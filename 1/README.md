Необходимые для работы библиотеки и их версии указаны в файле requirements.txt

Модель содзается при помощи файла /src-train/Train.ipynb
Данные для обучения (файл L.csv) находится также в /src-train

Для работы с табличными данными я использовал pandas. Классы заявок, которые необходимо было предсказать, я преобразовал при помощи one hot encoding'а.

Для обработки текста я использовал BERT, заранее обученный на русском языке, на входных данных BERT не обучался (обучение BERT занимало больше времени и в итоге снижало точность предсказаний). Я пробовал создавать токены из часто используемых терминов ЖКХ, такие как ГВС, ХВС и так далее, но это также уменьшало точность предсказаний. В финальной версии я использую стандартный tokenizer.

Для предсказания первой категории (Group) я использовал простую модель из двух Linear слоев и Dropout. На вход первого слоя приходили значения которые возвращал BERT. У второго слоя столько выходов, сколько разных классов нашел One-hot encoder при обработке данных.

При обучении вызывается функция train, затем функция evaluate. Если evaluate возвращает лосс меньше чем лучший, то модель сохраняется на жесткий диск. 

После обучения модели я предсказываю значения на тестовой выборке и сравниваю с реальными ответами. В результате у меня получилась точность более 95% при предсказании Group.

При предсказании Cat я использую тот же способ, но у модели в два раза больше hidden слой (1024, у прошлой модели было 512 ).

После обучения я сохраняю путь до первой и второй модели, а также объекты one-hot encoder'ов при помощи модуля pickle. Это нужно для того чтобы позже преобразовать предсказанные значения в текстовые значения. 
